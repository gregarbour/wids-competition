View(df)
colSums(df)
df <- data.frame(x1 = abs(rnorm(100)),
x2 = runif(100),
x3 = abs(rnorm(100)))
df$sum <- df$x1 + df$x2 + df$x3
df <- transmute(df, x1 = x1/sum,
x2 = x2/sum,
x3 = x3/sum)
rowSums(df)
library(readr)
library(GGally)
library(tidyverse)
library(scales)
library(caret)
library(e1071)
df <- read_csv("Desktop/Survey Paper/training_v2.csv")
set.seed(43)
trainIndex <- createDataPartition(df$hospital_death, p = .7, list = F)
test <- df[-trainIndex,]
df <- df[trainIndex[1:length(trainIndex)],]
#Toy functions to speed up exploration
sumNA <- function(x){sum(is.na(x))}
ptab <- function(x){sort(round(prop.table(table(x)),2))}
#Missingness
missing <- sapply(df, FUN = sumNA)
missing <- as.data.frame(missing)
missing$percent <- percent(missing$missing/nrow(df))
missing %>% arrange(percent) %>% ggplot(aes(x = percent)) %>% geom_histogram()
missing
missing %>% arrange(missing) %>% ggplot(aes(x = percent)) %>% geom_histogram()
missing %>% arrange(missing) %>% ggplot(aes(x = percent)) + geom_histogram()
missing %>% arrange(missing) %>% ggplot(aes(x = percent)) + geom_histogram(stat = 'count')
View(missing)
missing %>% arrange(missing)
missing$percent <- missing$missing/nrow(df)
missing %>% arrange(missing) %>% ggplot(aes(x = percent)) + geom_density()
missing %>% arrange(missing) %>% ggplot(aes(x = percent)) + geom_density(fill = 'coral') +
scale_x_continuous(labels = scales::percent_format(accuracy = 1))
library(readr)
library(GGally)
library(tidyverse)
library(scales)
library(caret)
library(e1071)
df <- read_csv("Desktop/Survey Paper/training_v2.csv")
set.seed(43)
trainIndex <- createDataPartition(df$hospital_death, p = .7, list = F)
test <- df[-trainIndex,]
df <- df[trainIndex[1:length(trainIndex)],]
#Toy functions to speed up exploration
sumNA <- function(x){sum(is.na(x))}
ptab <- function(x){sort(round(prop.table(table(x)),2))}
#Missingness
missing <- sapply(df, FUN = sumNA)
missing <- as.data.frame(missing)
missing$percent <- missing$missing/nrow(df)
sum(missing$missing) / (nrow(df)*ncol(df))
#Looking at individual variables
n_distinct(df$hospital_id)
n_distinct(df$encounter_id)
n_distinct(df$patient_id)
ptab(df$hospital_death)
df %>% ggplot(aes(age)) + geom_histogram()
hist(df$bmi)
table(df$elective_surgery, useNA = 'ifany')
sort(prop.table(table(df$hospital_admit_source)))*100
n_distinct(df$icu_id)
View(df)
names(df)[1:20]
head(df$icu_stay_type)
head(df$icu_type)
sumNA(df$d1_temp_max)
hist(df$d1_temp_max)
library(readr)
library(GGally)
library(tidyverse)
library(scales)
library(caret)
library(e1071)
df <- read_csv("Desktop/Survey Paper/training_v2.csv")
set.seed(43)
trainIndex <- createDataPartition(df$hospital_death, p = .7, list = F)
test <- df[-trainIndex,]
df <- df[trainIndex[1:length(trainIndex)],]
#Toy functions to speed up exploration
sumNA <- function(x){sum(is.na(x))}
ptab <- function(x){sort(round(prop.table(table(x)),2))}
#Missingness
missing <- sapply(df, FUN = sumNA)
missing <- as.data.frame(missing)
missing$percent <- missing$missing/nrow(df)
sum(missing$missing) / (nrow(df)*ncol(df)) # 33% of the values in the ENTIRE dataset are missing
dbinom(x = 1, size = 4, prob = 0.1)
source('~/Desktop/Survey Paper/Data Exploration.R', echo=TRUE)
#Test theory about some values. E.g. if multiple variables measure almost the same thing, can be
# combined together using a rule and therefore reduce overall missingness
heart <- df %>% select(d1_diasbp_invasive_max, d1_diasbp_max, d1_diasbp_noninvasive_max) %>%
mutate(inv_na = is.na(d1_diasbp_invasive_max),
either_na = is.na(d1_diasbp_max),
noninv_na = is.na(d1_diasbp_noninvasive_max))
View(heart)
a = F
b = F
a & b
b = T
a & b
a = T
a
b
a & b
# combined together using a rule and therefore reduce overall missingness
heart <- df %>% select(d1_diasbp_invasive_max, d1_diasbp_max, d1_diasbp_noninvasive_max) %>%
mutate(inv_na = is.na(d1_diasbp_invasive_max),
either_na = is.na(d1_diasbp_max),
noninv_na = is.na(d1_diasbp_noninvasive_max),
none_na = (inv_na & either_na & noninv_na))
View(heart)
#Test theory about some values. E.g. if multiple variables measure almost the same thing, can be
# combined together using a rule and therefore reduce overall missingness
heart <- df %>% select(d1_diasbp_invasive_max, d1_diasbp_max, d1_diasbp_noninvasive_max) %>%
mutate(inv_na = is.na(d1_diasbp_invasive_max),
either_na = is.na(d1_diasbp_max),
noninv_na = is.na(d1_diasbp_noninvasive_max),
none_na = (inv_na | either_na | noninv_na))
heart <- df %>% select(d1_diasbp_invasive_max, d1_diasbp_max, d1_diasbp_noninvasive_max) %>%
mutate(inv_na = is.na(d1_diasbp_invasive_max),
either_na = is.na(d1_diasbp_max),
noninv_na = is.na(d1_diasbp_noninvasive_max),
none_na = (inv_na | either_na | noninv_na)) %>%
filter(none_na == T)
sumNA(df$d1_diasbp_invasive_max)
sumNA(df$d1_diasbp_max)
library(tidyverse)
library(caret)
df <- read_csv("Desktop/Survey Paper/train v3.csv")
names(df)
sum(df$encounter_id == df$patient_id)
n_distinct(df$encounter_id)
library(tidyverse)
library(caret)
df <- read_csv("Desktop/Survey Paper/train v2.csv")
names(df)
df <- select(df, -X1, -index, -patient_id)
zero_vars <- caret::nearZeroVar(df)
names(df)[zero_vars]
hist(df$h1_albumin_diff)
hist(df$aids)
table(df$aids)
prop.table(table(df$aids))
zero_names = names(df)[zero_vars]
hist(df$h1_albumin_diff)
for(name in zero_names){
print(name)
print(prop.table(df[name]))
}
name
for(name in zero_names){
print(name)
print(prop.table(table(df[name])))
}
zero_names = names(df)[zero_vars]
hist(df$h1_albumin_diff)
for(name in zero_names[1:10]){
print(name)
print(prop.table(table(df[name])))
}
for(name in zero_names[11:36]){
print(hist(df[name]))
}
name
hist(df[name])
df[name]
# The first ten vars (non-diff vars) all have 97% a single level of the variable
for(name in zero_names[11:36]){
print(class(df[name]))
}
df[name]
hist(df[zero_names[i]])
# The first ten vars (non-diff vars) all have 97% a single level of the variable
i = 11
hist(df[zero_names[i]])
df[zero_names[i]]
# The first ten vars (non-diff vars) all have 97% a single level of the variable
i = 12
hist(df[zero_names[i]])
# The first ten vars (non-diff vars) all have 97% a single level of the variable
i = 15
hist(df[zero_names[i]])
hist(as.vector(df[zero_names[i]]))
df[zero_names[i]]
as.vector(df[zero_names[i]])
as.numeric(df[zero_names[i]])
class(df)
zero_names
# The first ten vars (non-diff vars) all have 97% a single level of the variable
hist(df$d1_glucose_diff)
# The first ten vars (non-diff vars) all have 97% a single level of the variable
hist(df$d1_glucose_diff)
hist(df$d1_hematocrit_diff)
hist(df$d1_lactate_diff)
hist(df$d1_platelets_diff)
# The first ten vars (non-diff vars) all have 97% a single level of the variable
hist(df$d1_glucose_diff)
#Remove the 36 suggested vars
df <- select(df, -zero_vars)
#Remove the 36 suggested vars
df <- select(df, -all_of(zero_vars))
df <- read_csv("Desktop/Survey Paper/train v2.csv")
df <- select(df, -X1, -index, -patient_id)
#### Remove correlated predictors ####
library(corrplot)
correlations <- cor(df, use = 'pairwise.complete.obs')
#### Remove correlated predictors ####
# Create df with numeric vars only
sum2 <- function(x){n_distinct(x) > 2}
aa <- summarise_if(df, .predicate = is.numeric, sum2) %>% t()
vars <- row.names(aa)[aa[,1]]
vars <- vars[-(1:3)]
vars <- setdiff(vars, names(df)[zero_vars])
df2 <- select(df, vars)
vars
aa <- summarise_if(df, .predicate = is.numeric, sum2) %>% t()
aa
zero_names
#Remove the 36 suggested vars
df <- select(df, -all_of(zero_names))
aa <- summarise_if(df, .predicate = is.numeric, sum2) %>% t()
aa[,1]
vars <- row.names(aa)[aa[,1]]
vars
vars <- vars[-(1:3)] #Remove id's and age
vars
df2 <- select(df, all_of(vars))
correlations <- cor(df2, use = 'pairwise.complete.obs')
correlations <- as.data.frame(correlations)
corr_90p <- sapply(correlations, FUN = function(x)sum(x > 0.9))
corr_90p
#Using algorithm outlines in Kuhn book, these are the predictors recommended for elimination
highCorr <- findCorrelation(correlations, cutoff = 0.9) #37% of the columns!!!!
highCorr
length(highCorr)#37% of the columns!!!!
length(highCorr)/nrow(df)#37% of the columns!!!!
length(highCorr)/ncol(df)#37% of the columns!!!!
#Try a higher cutoff value
highCorr2 <- findCorrelation(correlations, cutoff = 0.95)
length(highCorr2)/ncol(df2) #Still 27% (of the numeric columns, that is)
#Try an even higher cutoff value
highCorr3 <- findCorrelation(correlations, cutoff = 0.975)
length(highCorr3)/ncol(df2) #20% !!!! Got a lot of correlated shit here, jeez
highCorr3
names(df2)[highCorr3]
# Drop bars at the 95% cutoff
df <- df2 %>% select(-all_of(highCorr2))
write.csv(df, 'train_v3.csv')
highCorr2
names(df2)[highCorr2]
write.csv(df, 'train_v3.csv')
gewd()
getwd()
write.csv(df, 'Desktop/Survey Paper/train_v3.csv')
library(tidyverse)
library(caret)
df <- read_csv("Desktop/Survey Paper/train v2.csv")
df <- select(df, -X1, -index, -patient_id)
#### Which variables are extremely skewed towards only one value
zero_vars <- caret::nearZeroVar(df)
# 36 of these! Good news is they're mostly the 'diff' variables that we created, where the diff value is zero
zero_names = names(df)[zero_vars]
#Remove the 36 suggested vars
df <- select(df, -all_of(zero_names))
#### Remove correlated predictors ####
# Create df with numeric vars only
sum2 <- function(x){n_distinct(x) > 2}
aa <- summarise_if(df, .predicate = is.numeric, sum2) %>% t()
vars <- row.names(aa)[aa[,1]]
vars <- vars[-(1:3)] #Remove id's and age
df2 <- select(df, all_of(vars))
library(corrplot)
correlations <- cor(df2, use = 'pairwise.complete.obs')
correlations <- as.data.frame(correlations)
#Try a higher cutoff value
highCorr2 <- findCorrelation(correlations, cutoff = 0.95)
df <- df %>% select(-all_of(highCorr2))
write.csv(df, 'Desktop/Survey Paper/train_v3.csv')
df$bmi[1:10]
class(df$bmi)
table(df$apache_2_diagnosis)
type(df$apache_2_diagnosis)
class(df$apache_2_diagnosis)
library(readr)
library(GGally)
library(tidyverse)
library(scales)
library(caret)
library(e1071)
df <- read_csv("Desktop/Survey Paper/training_original.csv")
set.seed(43)
df$apache_2_diagnosis
df$apache_3j_diagnosis
table(df$apache_3j_diagnosis)
n_distinct(df$apache_2_diagnosis)
n_distinct(df$apache_3j_diagnosis)
head(names(Df))
head(names(df))
head(names(df), 10)
table(df$gender)
table(df$ethnicity)
sum(is.na(df$apache_2_diagnosis))
sum(is.na(df$apache_3j_diagnosis))
library(readr)
library(GGally)
library(tidyverse)
library(scales)
library(caret)
library(e1071)
df <- read_csv("Desktop/Survey Paper/training_original.csv")
set.seed(43)
trainIndex <- createDataPartition(df$hospital_death, p = .7, list = F)
test <- df[-trainIndex,]
df <- df[trainIndex[1:length(trainIndex)],]
#Toy functions to speed up exploration
sumNA <- function(x){sum(is.na(x))}
ptab <- function(x){sort(round(prop.table(table(x)),2))}
##### Missingness ######
missing <- sapply(df, FUN = sumNA)
missing <- as.data.frame(missing)
missing$percent <- missing$missing/nrow(df)
sum(missing$missing) / (nrow(df)*ncol(df)) # 33% of the values in the ENTIRE dataset are missing
missing %>% arrange(missing) %>% ggplot(aes(x = percent)) + geom_density(fill = 'coral') +
scale_x_continuous(labels = scales::percent_format(accuracy = 1))
missing
hist(missing$percent)
hist(missing$percent, main = 'Missingness proportion')
summary(df$d1_diasbp_min)
names(df)
n_distinct(df$apache_2_diagnosis)
library(dplyr)
n_distinct(df$apache_2_diagnosis)
n_distinct(df$apache_3j_diagnosis)
df$apache_3j_diagnosis[1:100]
hist(df$apache_3j_diagnosis)
hist(df$apache_3j_diagnosis, breaks = 200)
hist(prop.table(table(df$apache_3j_diagnosis)))
hist(prop.table(table(df$apache_3j_diagnosis)), breaks = 20)
hist(prop.table(table(df$apache_3j_diagnosis)), breaks = 30)
hist(table(df$apache_3j_diagnosis))
hist(table(df$apache_3j_diagnosis), breaks = 20)
hist(table(df$apache_3j_diagnosis), breaks = 30)
hist(table(df$apache_3j_diagnosis), breaks = 200)
500/64000
5/642
library(tidyverse)
library(caret)
df <- read_csv("Desktop/Survey Paper/train v2.csv")
df <- select(df, -X1, -index, -patient_id)
#### Which variables are extremely skewed towards only one value
zero_vars <- caret::nearZeroVar(df)
# 36 of these! Good news is they're mostly the 'diff' variables that we created, where the diff value is zero
zero_names = names(df)[zero_vars]
hist(df$h1_albumin_diff)
for(name in zero_names[1:10]){
print(name)
print(prop.table(table(df[name])))
}
#Remove the 36 suggested vars
df <- select(df, -all_of(zero_names))
#### Remove correlated predictors ####
# Create df with numeric vars only
sum2 <- function(x){n_distinct(x) > 2}
aa <- summarise_if(df, .predicate = is.numeric, sum2) %>% t()
aa
table(df$apache_post_operative)
head(df)[1:100]
numeric_vars <- summarise_if(df, .predicate = is.numeric, sum2) %>% t()
numeric_vars <- row.names(numeric_vars)[numeric_vars[,1]]
numeric_vars <- numeric_vars[-(1:3)] #Remove id's and age
df2 <- select(df, all_of(numeric_vars))
library(corrplot)
correlations <- cor(df2, use = 'pairwise.complete.obs')
correlations <- as.data.frame(correlations)
corr_90p <- sapply(correlations, FUN = function(x)sum(x > 0.9))
#Using algorithm outlines in Kuhn book, these are the predictors recommended for elimination
highCorr <- findCorrelation(correlations, cutoff = 0.9)
length(highCorr)/ncol(df) #33% of the columns!!!!
#Try a higher cutoff value
highCorr2 <- findCorrelation(correlations, cutoff = 0.95)
length(highCorr2)/ncol(df2) #Still 27% (of the numeric columns, that is)
#Try an even higher cutoff value
highCorr3 <- findCorrelation(correlations, cutoff = 0.975)
length(highCorr3)/ncol(df2) #20% !!!! Got a lot of correlated shit here, jeez
names(df2)[highCorr3]
# Drop bars at the 95% cutoff
df <- df %>% select(-all_of(highCorr2))
write.csv(df, 'Desktop/Survey Paper/train_v3.csv')
highCorr2
names(df)[highCorr2]
highCorr2
range(highCorr2)
names(df2)[highCorr2]
names(df2)[sort(highCorr2)]
library(tidyverse)
library(corrplot)
library(caret)
df <- read_csv("~/Desktop/WiDS Competition/Code/Data Files/df_all_pt1.csv")
df <- df %>% filter(is_train == 1) %>%  select(-X1, -index, -is_train)
#### Which variables are extremely skewed towards only one value
zero_vars <- caret::nearZeroVar(df)
df <- read_csv("~/Desktop/WiDS Competition/Code/Data Files/df_all_pt1.csv")
df <- read_csv("~/Desktop/WiDS Competition/Data Files/df_all_pt1.csv")
df <- df %>% filter(is_train == 1) %>%  select(-X1, -index, -is_train)
#### Which variables are extremely skewed towards only one value
zero_vars <- caret::nearZeroVar(df)
zero_names <- names(df)[zero_vars]
for(var in zero_names){
print(i)
for(i in 1:10){
print(i)
}
for(var in zero_names){
print(prop.table(table(df[var]))
}
zero_names
var
for(i in zero_names){
for(i in 1:10){
print(i)
}
for(i in zero_names){
print(prop.table(table(df[i]))
}
for(i in zero_vars){
print(i)
}
for(i in zero_names){
print(i)
}
i
table(df[i])
for(i in zero_names){
print(prop.table(table(df[i])))
}
i = 1
var <- zero_names[i]
hist(df[var])
i <- i + 1
var
df[var]
hist(df$readmission_status)
i <- i + 1
var <- zero_names[i]
hist(df[var])
var
df[var]
i <- i + 1
var <- zero_names[i]
hist(df[var])
hist(c(df[var]))
c(df[var])
hist(as.numeric(df[var]))
zero_names
hist(df$aids)
table(df$aids)
hist(df$d1_glucose_min_max)
hist(df$solid_tumor_with_metastasis)
hist(df$d1_glucose_min_max)
hist(df$d1_hemaglobin_min_max)
hist(df$d1_hematocrit_min_max)
hist(df$d1_lactate_min_max)
ggplot(df, aes(x = d1_lactate_min_max)) + geom_histogram()
summary(df$d1_lactate_min_max)
ggplot(df, aes(x = d1_lactate_min_max)) + geom_histogram() + xlab('Lactate Min/Max')
ggplot(df, aes(x = d1_lactate_min_max)) + geom_histogram() + xlab('Lactate Min/Max') +
ylab('Count') +
ggtitle('Histogram of Lactate Min/Max') +
theme(plot.title = element_text(hjust = 0.5))
library(table1)
table1(~ factor(aids), data = df)
# Table1 for Aids variable
aids <- data.frame(AIDS = factor(df$aids))
table1(~ AIDS, data = aids)
library(ggplot2)
library(readr)
library(dplyr)
library(table1)
library(corrplot)
setwd("~/Desktop/WiDS Competition")
#### Random Forest Tuning & Evaluation ####
rf <- read_csv("Model Files/All RF Results.csv")
# Showing error bar spread of AUC estimates from each candidate parameter set
rf1 <- rf %>% arrange(AUC) %>%
mutate(index = 1:n(),
y_max = AUC + std_dev,
y_min = AUC - std_dev)
rf2 <- rf1 %>% filter(max_features == 100, n_estimators == 50, index != 57)
#Candidate models using the 'simplest model within 1 std error of the best' approach
rf1 %>% filter(AUC > 0.8808738, index != 57) %>%
ggplot(aes(x = max_features, y = AUC, colour = factor(n_estimators))) +
geom_point() +
facet_grid(~max_depth)
#Candidate models using the 'simplest model within 1 std error of the best' approach
rf1 %>% filter(AUC > 0.8808738, index != 57) %>%
ggplot(aes(x = max_features, y = AUC, colour = factor(n_estimators))) +
geom_point() +
facet_grid(~max_depth) +
labs(color = "Number of Estimators")
#Candidate models using the 'simplest model within 1 std error of the best' approach
rf1 %>% filter(AUC > 0.8808738, index != 57) %>%
ggplot(aes(x = max_features, y = AUC, colour = factor(n_estimators))) +
geom_point() +
facet_grid(~max_depth) +
labs(color = "Number of Estimators") + xlab('Number of Estimators')
